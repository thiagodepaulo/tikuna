{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer\n",
    "from transformers.models.gpt2.tokenization_gpt2 import bytes_to_unicode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def text_iterator(directory):\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".txt\"):\n",
    "            file_path = os.path.join(directory, filename)\n",
    "            with open(file_path, \"r\") as file:\n",
    "                text = file.read()\n",
    "                yield text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 26.0/26.0 [00:00<00:00, 4.52kB/s]\n",
      "config.json: 100%|██████████| 665/665 [00:00<00:00, 99.6kB/s]\n",
      "vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 3.06MB/s]\n",
      "merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 2.00MB/s]\n",
      "tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 2.97MB/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "bytes_to_unicode = bytes_to_unicode()\n",
    "unicode_to_byte_map = {v: k for k, v in bytes_to_unicode.items()}\n",
    "base_vocab = list(unicode_to_byte_map.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignored unknown kwargs option initial_vocab\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "directory = \"../dataset/raw_text/\"\n",
    "new_tokenizer = tokenizer.train_new_from_iterator(text_iterator(directory), vocab_size=50257, initial_vocab=base_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "ids = new_tokenizer(\"NÜXÍRAXÜ̃XÜ̃ I TUPANAARÜ ORE \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NÜXÍRAXÜ̃XÜ̃ I TUPANAARÜ ORE \n"
     ]
    }
   ],
   "source": [
    "decoded_text = new_tokenizer.decode(ids['input_ids'])\n",
    "print(decoded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer config file saved in /tmp/tmp94ctzvrr/tokenizer_config.json\n",
      "Special tokens file saved in /tmp/tmp94ctzvrr/special_tokens_map.json\n",
      "Uploading the following files to thiagodepaulo/gpt2-tikuna: tokenizer.json,merges.txt,vocab.json,tokenizer_config.json,special_tokens_map.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/thiagodepaulo/gpt2-tikuna/commit/a151e058ff4a6ba30563c0be63f82166744c71fc', commit_message='Upload tokenizer', commit_description='', oid='a151e058ff4a6ba30563c0be63f82166744c71fc', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ckpt = \"gpt2-tikuna\"\n",
    "org = 'unb'\n",
    "\n",
    "new_tokenizer.push_to_hub(model_ckpt, org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "tokenizer_config.json: 100%|██████████| 234/234 [00:00<00:00, 19.0kB/s]\n",
      "vocab.json: 100%|██████████| 981k/981k [00:00<00:00, 2.13MB/s]\n",
      "merges.txt: 100%|██████████| 639k/639k [00:00<00:00, 1.41MB/s]\n",
      "tokenizer.json: 100%|██████████| 2.47M/2.47M [00:00<00:00, 4.15MB/s]\n",
      "special_tokens_map.json: 100%|██████████| 99.0/99.0 [00:00<00:00, 39.5kB/s]\n",
      "loading file vocab.json from cache at /home/thiagodepaulo/.cache/huggingface/hub/models--thiagodepaulo--gpt2-tikuna/snapshots/a151e058ff4a6ba30563c0be63f82166744c71fc/vocab.json\n",
      "loading file merges.txt from cache at /home/thiagodepaulo/.cache/huggingface/hub/models--thiagodepaulo--gpt2-tikuna/snapshots/a151e058ff4a6ba30563c0be63f82166744c71fc/merges.txt\n",
      "loading file tokenizer.json from cache at /home/thiagodepaulo/.cache/huggingface/hub/models--thiagodepaulo--gpt2-tikuna/snapshots/a151e058ff4a6ba30563c0be63f82166744c71fc/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at /home/thiagodepaulo/.cache/huggingface/hub/models--thiagodepaulo--gpt2-tikuna/snapshots/a151e058ff4a6ba30563c0be63f82166744c71fc/special_tokens_map.json\n",
      "loading file tokenizer_config.json from cache at /home/thiagodepaulo/.cache/huggingface/hub/models--thiagodepaulo--gpt2-tikuna/snapshots/a151e058ff4a6ba30563c0be63f82166744c71fc/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "reload_tokenizer = AutoTokenizer.from_pretrained(f\"thiagodepaulo/{model_ckpt}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
